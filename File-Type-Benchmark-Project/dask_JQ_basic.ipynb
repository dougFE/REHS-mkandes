{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informed-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit\n"
    "from dask_jobqueue import SLURMCluster\n",
    "cluster = SLURMCluster() #add arguments if want a cluster outside of the set up defult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "running-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(3) #scale to however many nodes wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "virgin-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "concerned-julian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.22.254.152:46209</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.22.254.152:8787/status' target='_blank'>http://10.22.254.152:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>3</li>\n",
       "  <li><b>Cores: </b>72</li>\n",
       "  <li><b>Memory: </b>300.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.22.254.152:46209' processes=3 threads=72, memory=300.00 GB>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "intended-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, sleep\n",
    "\n",
    "from distributed import Client, LocalCluster, as_completed\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "\n",
    "import os\n",
    "import dask\n",
    "import dask.dataframe as dd  # Important line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "crude-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment_runs = 1  # Number of times to repeat r/w tests\n",
    "\n",
    "#sourceDir = '/oasis/scratch/comet/dougfe/temp_project/df_benchmarking/daskBench/benchHome/'  # Location of outputs and source csv files. Copy output of $pwd if unsure of what to put here\n",
    "\n",
    "sourceDir = 'benchHome/'\n",
    "\n",
    "df_fileprefix = 'bench'  # Central name for all files produced during benchmark, includes '.' for later attachment to extensions in extensionList\n",
    "\n",
    "df_source = ['_1week.csv','_2week.csv','_4week.csv'] # List of dataframes to run analysis on, will help name folder\n",
    "\n",
    "\n",
    "def write_bench(df_in, filename, reportPath):\n",
    "    for i in range(experiment_runs):\n",
    "        reportFile = open(reportPath, 'a')\n",
    "        savetime = time()\n",
    "        dd.to_csv(df_in, filename)\n",
    "        reportFile.write(str(time()-savetime)+' ')\n",
    "        reportFile.close()\n",
    "\n",
    "def read_bench(readSource, reportPath):\n",
    "    reportFile = open(reportPath, 'a')\n",
    "    for i in range(experiment_runs):\n",
    "        savetime = time()\n",
    "        tempDF = dd.read_csv(readSource, dtype = 'object', low_memory = False)\n",
    "        reportFile.write(str(time()-savetime)+' ')\n",
    "        tempDF = ''\n",
    "    reportFile.close()\n",
    "\n",
    "def writeParquetBench(df_in, filename, reportPath):\n",
    "    for i in range(experiment_runs):\n",
    "        #if os.path.exists(filename):\n",
    "        #    os.remove(filename)\n",
    "        reportFile = open(reportPath, 'a')\n",
    "        savetime = time()\n",
    "        dd.to_parquet(df_in, filename)\n",
    "        reportFile.write(str(time()-savetime)+' ')\n",
    "        reportFile.close()\n",
    "\n",
    "def readParquetBench(readSource, reportPath):\n",
    "    reportFile = open(reportPath, 'a')\n",
    "    for i in range(experiment_runs):\n",
    "        savetime = time()\n",
    "        tempDF = dd.read_parquet(readSource, dtype = 'object', low_memory = False)\n",
    "        reportFile.write(str(time()-savetime)+' ')\n",
    "        tempDF = ''\n",
    "    reportFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sunset-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dug ():\n",
    "    extension = 'csv'\n",
    "    for sourceCSV in df_source:   \n",
    "\n",
    "        testFilepath = sourceDir + df_fileprefix + sourceCSV[:-3] + extension  # Name+loc of output files\n",
    "        reportW = sourceDir + extension + '_w_' + sourceCSV[:-4] + '.txt'\n",
    "        currentDF = dd.read_csv(sourceDir+sourceCSV, dtype = 'object', low_memory = False)\n",
    "        currentDF\n",
    "        write_bench(currentDF, testFilepath, reportW)\n",
    "\n",
    "        reportR = sourceDir + extension + '_r_' + sourceCSV[:-4] + '.txt'\n",
    "        read_bench(sourceDir+sourceCSV, reportR)\n",
    "    \n",
    "    extension = 'parquet'\n",
    "    for sourceCSV in df_source:   \n",
    "\n",
    "        testFilepath = sourceDir + df_fileprefix + sourceCSV[:-3] + extension  # Name+loc of output files\n",
    "        reportW = sourceDir + extension + '_w_' + sourceCSV[:-4] + '.txt'\n",
    "        currentDF = dd.read_csv(sourceDir+sourceCSV, dtype = 'object', low_memory = False)\n",
    "        writeParquetBench(currentDF, testFilepath, reportW)\n",
    "\n",
    "        reportR = sourceDir + extension + '_r_' + sourceCSV[:-4] + '.txt'\n",
    "        readParquetBench(sourceDir+sourceCSV[:-3]+'parquet', reportR)\n",
    "    return('Benchmark complete.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "direct-journalist",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "First input to submit must be a callable function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, func, key, workers, resources, retries, priority, fifo_timeout, allow_other_workers, actor, actors, pure, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \"\"\"\n\u001b[1;32m   1553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"First input to submit must be a callable function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0mactor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mactors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: First input to submit must be a callable function"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "       \n",
    "    #client.upload_file(\"/oasis/scratch/comet/caolinnh/temp_project/test.py\")\n",
    "    \n",
    "    future = client.submit(dug())\n",
    "    #future = client.submit(exec(open(\"test.py\").read())\n",
    "        \n",
    "    #open(\"/oasis/scratch/comet/caolinnh/temp_project/1WBenchComet.py\")).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-consensus",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


